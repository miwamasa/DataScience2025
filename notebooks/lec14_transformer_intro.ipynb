{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGuGYFz21OCD"
      },
      "source": [
        "# Lecture14 LLM時代のデータサイエンスとは\n",
        "\n",
        "【2024年度版】\n",
        "\n",
        "ChatGPTや大規模言語モデル(LLM)の元となったTransformerの概要やHuggingFaceを利用した実行例を試し、最後に、データサイエンスとの関係性についていくつか話題を実例を含めながら紹介する。\n",
        "\n",
        "- 深層学習から発展し、ChatGPTなどのもとになった、Transformerについて、動かしてみる。（時系列データ予測もやってみる）\n",
        "- LLMの活用例\n",
        " - CSVデータエージェント\n",
        " - Q&A with sources\n",
        "\n",
        "【注意】実行環境は様々なライブラリのバージョンに依存します、将来の動作は全く保証されません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYPSl0pX1UYW"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "ここでは「機械学習エンジニアのためのTransfomers」の例を実際に動かしてみます。\n",
        "\n",
        "ここでは、HuggingFaceというオープンソースのモデルやデータをあつめたハブからpipeline()関数を使ってローカルにtransformerのモデルをダウンロード（実際にはcolabの仮想マシンに）して使っています。\n",
        "\n",
        "\n",
        "https://github.com/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzfOI-uRSa9U"
      },
      "source": [
        "## Transformerの歴史\n",
        "Transformerとは、深層学習のアーキテクチャの一つであり、Googleにより発明されました。それ以降、BERTやGPTなどが開発され、現在のGPT3.5やGPT4すなわちChatGPTのもとになるTransformerが開発されてきました。\n",
        "\n",
        "![Transformerの歴史](https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter01_timeline.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go2JLZ3KTZyt"
      },
      "source": [
        "## 「エンコーダー・デコーダ」アーキテクチャ\n",
        "\n",
        "深層学習の中で、RNN(再帰型ニューラルネットワーク）を用いて、入力文字の順序を持つ列を、状態にエンコードする、エンコーダーブロックと、状態からこれを、出力文字列に、やはりRNNを用いてでコードするデコードブロックを組み合わせたものが、トランスフォーマの基本となるアーキテクチャです。\n",
        "![](https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter01_enc-dec.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers accelerate scipy safetensors sacremoses"
      ],
      "metadata": {
        "id": "H84BI_nBbaVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY8nxGF2HnX3"
      },
      "source": [
        "例題とするテキスト（英語）です、~~Amazon~~EC業者への問い合わせのメールのようです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "but2rW8X1tav"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
        "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
        "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
        "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
        "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
        "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
        "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58oXRCh_8B3u"
      },
      "source": [
        "## テキスト分類の例\n",
        "pipleline関数に、タスク種別を引数に指定すると、適切なモデルを自動選択して、モデルのインスタンスをcolab仮想マシンに生成します。（具体的なモデルを指定することもできます）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRm1NghD1RzQ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "#distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
        "\n",
        "classifier = pipeline(\"text-classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XW4AAVH8a8f"
      },
      "source": [
        "具体的に選択されたモデルを表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qQKs2HI8a8f"
      },
      "outputs": [],
      "source": [
        "classifier.model.name_or_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPi8QpkX8a8g"
      },
      "outputs": [],
      "source": [
        "classifier.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-SYX6VZWaQG"
      },
      "source": [
        "このようにして、与えた文章に対して感情を分析してくれます。スコア付き。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMJXTQofWUHp"
      },
      "outputs": [],
      "source": [
        "classifier(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPWsS_NgHuvG"
      },
      "source": [
        "一度、モデルを作れば、あとは何度も別のテキストを試せます。ディープラーニングの父、ヒントン教授のインタビューから"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jREYx-dsHOhW"
      },
      "outputs": [],
      "source": [
        "text_hinton=\"\"\"Geoff Hinton\n",
        "\n",
        "Yes, I do. I strongly believe that. I strongly believe that when we eventually understand how the brain works,\n",
        " that's going to give us lots of psychological insight too. Just as understanding chemistry at the atomic level,\n",
        " understanding of how molecules bump into each other and what happens,\n",
        " gives us lots of insight into the gas laws.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bazYIMQiHWGY"
      },
      "outputs": [],
      "source": [
        "classifier(text_hinton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnL75oK74RvK"
      },
      "source": [
        "## 固有語の識別\n",
        "\n",
        "named entity(名前のあるエンティティ)の識別"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKqj26E62kWG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf\n",
        "\n",
        "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "outputs = ner_tagger(text)\n",
        "pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYhDNqCq4-55"
      },
      "source": [
        "## 質問に答える"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr6M5EI24Wa9"
      },
      "outputs": [],
      "source": [
        "\n",
        "reader = pipeline(\"question-answering\")\n",
        "question = \"What does the customer want?\"\n",
        "outputs = reader(question=question, context=text)\n",
        "pd.DataFrame([outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylfdfZg65H-B"
      },
      "source": [
        "## 要約をする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKBsI5D65BCk"
      },
      "outputs": [],
      "source": [
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "outputs = summarizer(text, max_length=60, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3pgjQL50tg"
      },
      "source": [
        "## 翻訳では、FuguMTを使ってみます。\n",
        "\n",
        "2024.5 なぜか壊れている。。。\n",
        "\n",
        "→以下で対応してみた\n",
        "`pip install -U transformers==4.30.2 `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q77hDtcK5LDG"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\n",
        "\n",
        "#例(高慢と偏見 Jane Austen)\n",
        "txt=\"\"\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
        " However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well\n",
        "  fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\n",
        "\"\"\"\n",
        "fugu_translator(txt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fugu_translator(outputs[0]['summary_text'])"
      ],
      "metadata": {
        "id": "pknoj-lKB9qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6VJi9U1IMcL"
      },
      "source": [
        "もう少し本格的な"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUwvhcz5Ig3P"
      },
      "outputs": [],
      "source": [
        "translator = pipeline(\"translation\", model='Hoax0930/marian-finetuned-kde4-en-to-ja_kftt')\n",
        "translator(outputs[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator(txt)"
      ],
      "metadata": {
        "id": "GLFGWUlpFfGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbShf2Vd-fBt"
      },
      "source": [
        "## 画像生成\n",
        "テキストから画像を生成する有名な、stablefusionも試せます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlkc0Nim-egM"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate scipy safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfuvwLfC9MrO"
      },
      "source": [
        "メモリ利用量が無料版のcolabを超えるかもしれません。その場合は、クラッシュします。がメモリがリセットされるので再実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmDE0kfyE6KS"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
        "import torch\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-2\"\n",
        "\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"The professor in panda outfits is teaching in a busy and packed class room. Some student is throwing spears to him\"\n",
        "image = pipe(prompt).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The professor in panda outfits is teaching in a busy and packed class room. Some student is throwing spears to him\"\n",
        "image = pipe(prompt).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "p394s8hTI80Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## langchainで動かしてみます"
      ],
      "metadata": {
        "id": "07wVHwTGQLa3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upnmFx4fn2XG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは、ローカル環境(colab)にモデルをダウンロードしてローカルに使ってみます。\n",
        "\n",
        "メモリ不足で、セッションがクラッシュする可能性もありますが、実行しなおせば大丈夫です汗"
      ],
      "metadata": {
        "id": "3dMVqXi7NYKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCOijd98E65_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "#from langchain import PromptTemplate,  LLMChain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from IPython.display import display\n",
        "\n",
        "model_id=\"bigscience/bloom-1b7\"\n",
        "llm = HuggingFacePipeline.from_model_id(model_id=model_id, task=\"text-generation\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "answer = llm_chain.run(question)\n",
        "display(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fugu_translator(answer)"
      ],
      "metadata": {
        "id": "okV38BurTfEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALiGejrl-gDD"
      },
      "source": [
        "一度でも、ロードが成功すれば、あとは大丈夫。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahHFb9Zh-kaH"
      },
      "outputs": [],
      "source": [
        "question = \"Why nuclear fusion is so difficult from enginnering perspective?\"\n",
        "\n",
        "display(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNTFqtWcd_xJ"
      },
      "source": [
        "### OpenAIのapiのkeyを持っている場合は。OpenAIの計算リソースが利用できる\n",
        "\n",
        "OpenAIの（超高速な）サーバーにあるモデルを使うことができます。ローカルメモリを消費しない。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQIVDa8bfQa_",
        "outputId": "89377373-cd9b-4eba-9d33-4ca8fe03c9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (0.1.59)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (23.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (8.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.46->langchain_openai) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai  langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3hB9E6eeRuT",
        "outputId": "55eaf4b1-e189-4320-c87d-8d6196203f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "# get a token: https://platform.openai.com/account/api-keys\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDifQVHejj2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "w6O_XmQsoALg",
        "outputId": "4dbcbb46-c0cd-4808-a9cb-a4b28e869dea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\" Firstly, we need to break down the word 'electroencephalography' into smaller parts. 'Electro' means electricity, 'encephalo' refers to the brain, and 'graphy' is the process of recording something. So, electroencephalography is the process of recording the electrical activity of the brain.\\n\\nIn simpler terms, electroencephalography (EEG) is a medical procedure that involves placing electrodes on a person's scalp to measure and record the electrical impulses produced by the brain. These electrodes detect and amplify the brain's electrical signals, which are then displayed as a graph or wave pattern on a computer screen. EEG is often used to diagnose and monitor conditions such as epilepsy, sleep disorders, and brain injuries. It can also be used in research to study brain function and activity.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "answer = llm_chain.run(question)\n",
        "display(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fugu_translator(answer)"
      ],
      "metadata": {
        "id": "zqVoj16JTr6C",
        "outputId": "2b5b66d9-22f7-4d05-c8b6-44723aecfc10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'まず「脳波」という言葉を より小さな部分に分解する必要があります 「エレクトロ」は電気 「脳」は脳を指し 「グラフ」は何かを記録するプロセスです エレクトロ脳波は脳の電気的活動を記録するプロセスです 簡単に言うと 脳波(EEG)とは脳が生み出す電気的インパルスを計測し記録する 電極を人の頭皮に貼り付ける医療手法です これらの電極は 脳の電気信号を検知して増幅し コンピュータのコンピュータに表示します 脳波は てんかん、睡眠障害、脳損傷などの状態を診断し監視するのによく使われます 脳機能や活動を研究する研究にも使われます'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIragcC9fokE"
      },
      "source": [
        "## 有料のOpenAIのapi_keyを持っていなくても、無料のHugging Faceの計算リソースを使うことができます。\n",
        "\n",
        "こちらも、HugginfFaceのもつ（たぶん、超高速の）サーバーを利用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0v6nzlPfxQ3",
        "outputId": "8bf248ef-7e0e-4252-c6fe-6f05d0f8fba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement from (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for from\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U from langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4PxfiZIf5I_",
        "outputId": "b0a0ff25-e2c7-42a4-dea6-b265b15aaa06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQBDR-QIf2Y3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "bPTzLamkHPHk",
        "outputId": "e4eaf7bd-6e36-4b5e-d690-a255f011091f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' Electroencephalography, or EEG, is a medical test that measures the electrical activity of the brain. The word \"electroencephalography\" comes from three parts:\\n\\n1. \"Electro\": This refers to electricity, as the test measures electrical signals.\\n2. \"Encephalo\": This is derived from the Greek word for brain.\\n3. \"Graphy\": This means to write, as the test produces a graph or recording of the brain\\'s electrical activity.\\n\\nSo, EEG is a test that records the electrical activity of the brain using electrodes placed on the scalp. These electrodes detect the tiny electrical impulses produced by neurons in the brain when they communicate with each other. The resulting EEG trace can provide valuable information about the brain\\'s electrical activity, which can help diagnose various neurological and psychiatric conditions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id , temperature=0.5,  max_length=64)\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "answer = llm_chain.run(question)\n",
        "display(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fugu_translator(answer)"
      ],
      "metadata": {
        "id": "kAy3u0ZYO-mj",
        "outputId": "d9821051-96bd-4c04-cc56-540bdab23896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': '脳波(Electroencephalography、EEG)とは、脳の電気的活動を測定する医学的検査である。「脳波(Electroencephalography)」とは、電気的信号を測定するための電気のことを指す。「脳波(Encephalo)」とは、ギリシャ語で「脳」を意味する。「脳波(Graphy)」とは、脳が電気的活動のグラフや記録を作成するときに、脳が脳に電気的活動を記録することを意味している。つまり、脳波は、頭皮に電極を配置して脳の電気的活動を記録するテストである。これらの電極は、脳内のニューロンが互いに通信する際に生じる小さな電気的インパルスを検出する。結果として生じる脳波の痕跡は、脳の電気的活動に関する貴重な情報を提供し、様々な神経学的および精神医学的状態を診断するのに役立つ。'}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}